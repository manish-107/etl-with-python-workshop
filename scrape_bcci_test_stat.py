# -*- coding: utf-8 -*-
"""scrape_bcci_test_stat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vV-PpDu4K3EDqJVvCymnCgrEXFcgCkkp

# 1. Scrape the data

### 1.1 Install and import the necessary packages
"""

!pip install psycopg[binary]

import os
import sys
from typing import List, Literal, Optional

import pandas as pd
import requests
from bs4 import BeautifulSoup
import psycopg
from psycopg.rows import dict_row

"""### 1.2 API Endpoints and other initializations"""

# Raw data source
MOST_RUNS_TEST_URL = "https://www.bcci.tv/getStats?platform=international&type=men&s_type=batting&slug=batting_most_runs&format=test"
MOST_RUNS_ODI_URL = "https://www.bcci.tv/getStats?platform=international&type=men&s_type=batting&slug=batting_most_runs&format=odi"
TOP_WICKET_TAKERS_ODI_URL = "https://www.bcci.tv/getStats?platform=international&type=men&s_type=bowling&slug=bowling_top_wicket_takers&format=odi"
TOP_WICKET_TAKERS_TEST_URL = "https://www.bcci.tv/getStats?platform=international&type=men&s_type=bowling&slug=bowling_top_wicket_takers&format=test"

OUT_DIR = os.path.join(os.getcwd(), "out")

Discipline = Literal["batting", "bowling"] # Typings for "batting" or "bowling"

jobs: list[tuple[str, str, Discipline]] = [
    (MOST_RUNS_TEST_URL, "bcci_test_most_runs", "batting"),
    (MOST_RUNS_ODI_URL, "bcci_odi_most_runs", "batting"),
    (TOP_WICKET_TAKERS_TEST_URL, "bcci_test_top_wickets", "bowling"),
    (TOP_WICKET_TAKERS_ODI_URL, "bcci_odi_top_wickets", "bowling"),
]

columns: dict[Discipline, list[str]] = {
"batting": [
        "Rank",
        "Name",
        "Matches",
        "Inns",
        "Avg",
        "SR",
        "HS",
        "Fours",
        "Sixes",
        "Fifties",
        "Centuries",
        "Runs",
    ],
    "bowling": [
        "Rank",
        "Name",
        "Matches",
        "Inns",
        "Avg",
        "Econ",
        "SR",
        "BBI",
        "Four_w",
        "Five_w",
        "Wkts",
    ],
}

# List of all the CSV files that will be created
saved_paths: List[str] = []

print("Initialzed the variables")

"""### 1.3 Let's get the data in it's raw format


"""

# Create output directory if it doesn't exist
os.makedirs(OUT_DIR, exist_ok=True)

# Store the HTML for each API
html_store: dict[str, str] = {}

# Process each API endpoint
for url, basename, kind in jobs:
    # Make HTTP request to BCCI API
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()  # Raise exception for HTTP errors

    # Parse JSON response from API
    payload = resp.json()  # Returns a JSON response with HTML content
    html = payload.get("html", None)
    if not html:
        print(f"The HTML data for {basename} does not exists!")
        continue  # Skip if no HTML data in response

    # Store the HTML for extraction
    html_store[basename] = html

    print(f"Got raw data for {basename}")

    # See the HTML if you wish (âš ï¸ The contents are too long!)
    # print(html)

"""### 1.4 Helper functions for cleaning the data"""

def normalize_label(text: str) -> str:
    """Normalize cricket statistics labels to consistent format."""
    # Clean up text: remove smart quotes, periods, and normalize case
    low = text.replace("\u2019", "'").replace(".", "").strip().lower()

    # Mapping dictionary to standardize label names
    mapping = {
        "matches": "Matches",
        "inns": "Inns",
        "avg": "Avg",
        "sr": "SR",
        "hs": "HS",
        "runs": "Runs",
        "4's": "Fours",
        "4s": "Fours",
        "6's": "Sixes",
        "6s": "Sixes",
        "50's": "Fifties",
        "50s": "Fifties",
        "100's": "Centuries",
        "100s": "Centuries",
        "econ": "Econ",
        "economy": "Econ",
        "wkts": "Wkts",
        "wickets": "Wkts",
        "bbi": "BBI",
        "best bowling": "BBI",
        "best": "BBI",
        "4w": "Four_w",
        "5w": "Five_w",
    }
    return mapping.get(low, text.strip())


def coerce_value(text: str):
    """Convert string values to appropriate data types (int, float, or string)."""
    try:
        if "." in text:  # Contains decimal point
            return float(text)
        return int(text)  # Integer value
    except Exception:
        return text.strip()  # Return original string if conversion fails

print("Initialized helper functions")

"""### 1.5 Extract the data from the HTML"""

def extract_data_from_html(html: str, kind: Discipline) -> tuple[list[dict], list[str]]:
    """Parse rows from the getStats API HTML snippet."""
    soup = BeautifulSoup(html, "lxml")

    # Find the statistics table in the HTML
    table = soup.select_one(".stats-data-table-player table")
    if table is None:
        return []

    records: list[dict] = []

    # Get first ranking player (Since it is a separate section in the HTML)
    records.append(get_first_rank_player(soup))

    # Process each row in the table
    for tr in table.select("tr"):
        tds = tr.find_all("td")
        if len(tds) < 3:  # Skip rows with insufficient data
            continue

        # Extract rank (serial number) from first column
        sn_el = tds[0].find(["h5", "h6"]) or tds[0]
        name_el = tds[1].find("h6") or tds[1]
        try:
            sn = int((sn_el.get_text(strip=True) or "0").strip())
        except Exception:
            sn = None

        # Extract player name from second column
        name = name_el.get_text(strip=True)
        row: dict = { "Rank": sn, "Name": name }

        # Extract statistics from remaining columns (matches, innings, avg, etc.)
        for td in tds[2:]:
            val_el = td.find("h6")  # Value element
            lab_el = td.find("span")  # Label element
            if not val_el or not lab_el:
                continue

            val_txt = val_el.get_text(strip=True)
            lab_txt = lab_el.get_text(strip=True)
            key = normalize_label(lab_txt)  # Normalize label (e.g., "4's" -> "Fours")
            row[key] = coerce_value(val_txt)  # Convert value to appropriate type

        records.append(row)

    # Ensure all records have all columns (fill missing with None)
    for r in records:
        for c in columns[kind]:
            if c not in r:
                r[c] = None

    return records

def get_first_rank_player(soup: BeautifulSoup):
    name_container = soup.select_one(".player-name-trw")
    stat_table = soup.select_one(".ranking-top-table table")
    if not name_container or not stat_table:
        print("Top ranking player not found!")
        return None
    name = " ".join(name_container.stripped_strings)

    stats = { "Rank": 1, "Name": name }
    for td in stat_table.select("td"):
        label = td.find("span").get_text(strip=True)
        value = td.find("p").get_text(strip=True)
        key = normalize_label(label)
        stats[key] = coerce_value(value)

    return stats


print("Initialzed extract function")

"""### 1.6 Store and display the data"""

from IPython.display import display

# Extract records and column names from HTML
# Iterate over jobs to correctly get 'basename' and 'discipline'
for url, basename, discipline in jobs:
  # Get the HTML that you stored before
  html = html_store[basename]

  # Calling extract_data_from_html will implicitly use the global 'columns' dict for parsing.
  records = extract_data_from_html(html, discipline) # No need to capture 'columns' returned by func if it's the global one

  if not records:
    print(f"No records found for {basename}!")
    continue

  # Use the global 'columns' dictionary with the current 'discipline' to get the list of column names
  df = pd.DataFrame.from_records(records, columns=columns[discipline])

  # If you wish to see the file
  print(f"--------{basename}----------")
  display(df,)
  print("\n\n\n")

  # Save data to CSV file
  path = os.path.join(OUT_DIR, f"{basename}.csv")
  df.to_csv(path, index=False)
  saved_paths.append(path)

"""# 2. Storing the data to the Database

### 2.1 Get the secrets

- Get the Database URL from https://console.neon.tech/
- Click on 'Secrets' ðŸ”‘ menu in the left side panel of Google Colab.
- Add a new Secret with the name `DATABASE_URL` and value will be the connection URL to the database.
- Enable the 'Notebook access' switch besides the secrets.
"""

from google.colab import userdata

DATABASE_URL = userdata.get('DATABASE_URL')

if not DATABASE_URL:
  raise Exception("DATABASE_URL not found! Follow the documentation above")

"""### 2.2 Create necessary tables"""

TABLES_TO_CREATE = [
    """
      CREATE TABLE IF NOT EXISTS bcci_odi_most_runs (
        rank INTEGER PRIMARY KEY,
        name TEXT NULL,
        matches INTEGER NULL,
        inns INTEGER NULL,
        avg DOUBLE PRECISION NULL,
        sr DOUBLE PRECISION NULL,
        hs DOUBLE PRECISION NULL,
        Fours INTEGER NULL,
        Sixes INTEGER NULL,
        Fifties INTEGER NULL,
        Centuries INTEGER NULL,
        runs INTEGER NULL
      );
    """,

    """
    CREATE TABLE IF NOT EXISTS bcci_odi_top_wickets (
      rank INTEGER PRIMARY KEY,
      name TEXT NULL,
      matches INTEGER NULL,
      inns INTEGER NULL,
      avg DOUBLE PRECISION NULL,
      econ DOUBLE PRECISION NULL,
      sr DOUBLE PRECISION NULL,
      bbi TEXT NULL,
      Four_w INTEGER NULL,
      Five_w INTEGER NULL,
      wkts INTEGER NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS bcci_test_most_runs (
      rank INTEGER PRIMARY KEY,
      name TEXT NULL,
      matches INTEGER NULL,
      inns INTEGER NULL,
      avg DOUBLE PRECISION NULL,
      sr DOUBLE PRECISION NULL,
      hs DOUBLE PRECISION NULL,
      Fours INTEGER NULL,
      Sixes INTEGER NULL,
      Fifties INTEGER NULL,
      Centuries INTEGER NULL,
      runs INTEGER NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS bcci_test_top_wickets (
      rank INTEGER PRIMARY KEY,
      name TEXT NULL,
      matches INTEGER NULL,
      inns INTEGER NULL,
      avg DOUBLE PRECISION NULL,
      econ DOUBLE PRECISION NULL,
      sr DOUBLE PRECISION NULL,
      bbi TEXT NULL,
      Four_w INTEGER NULL,
      Five_w INTEGER NULL,
      wkts INTEGER NULL
    );
    """
]

with psycopg.connect(DATABASE_URL, autocommit=True) as conn:
  with conn.cursor() as cur:
    for create_query in TABLES_TO_CREATE:
      cur.execute(create_query)
      print("Created table successfully!")

"""### 2.3 Utility functions"""

def to_int(v: str):
    try:
        return int(v)
    except Exception:
        return None


def to_float(v: str):
    try:
        return float(v)
    except Exception:
        return None


def to_text(v: str):
    v = v.strip()
    return v if v and v != "-" else None

"""## 2.4 Insert ODI Most Runs to the Database"""

INSERT_ODI_MOST_RUNS_SQL = """
  INSERT INTO bcci_odi_most_runs
    (rank, name, matches, inns, avg, sr, hs, Fours, Sixes, Fifties, Centuries, runs)
  VALUES
    (%(rank)s, %(name)s, %(matches)s, %(inns)s, %(avg)s, %(sr)s, %(hs)s, %(fours)s, %(sixes)s, %(fifties)s, %(centuries)s, %(runs)s)
  ON CONFLICT (rank) DO UPDATE SET
    name = EXCLUDED.name,
    matches = EXCLUDED.matches,
    inns = EXCLUDED.inns,
    avg = EXCLUDED.avg,
    sr = EXCLUDED.sr,
    hs = EXCLUDED.hs,
    Fours = EXCLUDED.Fours,
    Sixes = EXCLUDED.Sixes,
    Fifties = EXCLUDED.Fifties,
    Centuries = EXCLUDED.Centuries,
    runs = EXCLUDED.runs;
"""

csv_path = os.path.join(os.getcwd(), OUT_DIR, "bcci_odi_most_runs.csv")

# Check if the file exists
if not os.path.exists(csv_path):
    raise Exception(f"CSV not found: {csv_path}", file=sys.stderr)

print(f"CSV found {csv_path}")

# Connect to the Database
with psycopg.connect(DATABASE_URL, autocommit=True) as conn:
    # Create a cursor and open the CSV file
    with conn.cursor() as cur, open(csv_path, newline="", encoding="utf-8") as f:
        # Read the CSV file using pandas
        odi_most_runs_df = pd.read_csv(f)
        records_to_insert = []
        for row in odi_most_runs_df.itertuples(index=False):
            payload = {
                "rank": to_int(row.Rank),
                "name": to_text(row.Name),
                "matches": to_int(row.Matches),
                "inns": to_int(row.Inns),
                "avg": to_float(row.Avg),
                "sr": to_float(row.SR),
                "hs": to_float(row.HS),
                "fours": to_int(row.Fours),
                "sixes": to_int(row.Sixes),
                "fifties": to_int(row.Fifties),
                "centuries": to_int(row.Centuries),
                "runs": to_int(row.Runs),
            }

            # rank is required; skip rows without a valid rank
            if payload["rank"] is None:
                continue
            records_to_insert.append(payload)

        if records_to_insert:
            cur.executemany(INSERT_ODI_MOST_RUNS_SQL, records_to_insert)
            print(f"Successfully inserted {len(records_to_insert)} records into bcci_odi_most_runs.")

"""## 2.5 Insert Test Most Runs to the Database"""

INSERT_TEST_MOST_RUNS_SQL = """
  INSERT INTO bcci_test_most_runs
    (rank, name, matches, inns, avg, sr, hs, fours, sixes, fifties, centuries, runs)
  VALUES
    (%(rank)s, %(name)s, %(matches)s, %(inns)s, %(avg)s, %(sr)s, %(hs)s, %(fours)s, %(sixes)s, %(fifties)s, %(centuries)s, %(runs)s)
  ON CONFLICT (rank) DO UPDATE SET
    name = EXCLUDED.name,
    matches = EXCLUDED.matches,
    inns = EXCLUDED.inns,
    avg = EXCLUDED.avg,
    sr = EXCLUDED.sr,
    hs = EXCLUDED.hs,
    fours = EXCLUDED.fours,
    sixes = EXCLUDED.sixes,
    fifties = EXCLUDED.fifties,
    centuries = EXCLUDED.centuries,
    runs = EXCLUDED.runs;
"""

csv_path = os.path.join(os.getcwd(), OUT_DIR, "bcci_test_most_runs.csv")

# Check if the file exists
if not os.path.exists(csv_path):
    print(f"CSV not found: {csv_path}", file=sys.stderr)
    sys.exit(1)

print(f"CSV found {csv_path}")

# Connect to the Database
with psycopg.connect(DATABASE_URL, autocommit=True) as conn:
    # Create a cursor and open the CSV file
    with conn.cursor() as cur, open(csv_path, newline="", encoding="utf-8") as f:
        # Read the CSV file using pandas
        test_most_runs_df = pd.read_csv(f)
        records_to_insert = []
        for row in test_most_runs_df.itertuples(index=False):
            payload = {
                "rank": to_int(row.Rank),
                "name": to_text(row.Name),
                "matches": to_int(row.Matches),
                "inns": to_int(row.Inns),
                "avg": to_float(row.Avg),
                "sr": to_float(row.SR),
                "hs": to_float(row.HS),
                "fours": to_int(row.Fours),
                "sixes": to_int(row.Sixes),
                "fifties": to_int(row.Fifties),
                "centuries": to_int(row.Centuries),
                "runs": to_int(row.Runs),
            }

            # rank is required; skip rows without a valid rank
            if payload["rank"] is None:
                continue
            records_to_insert.append(payload)

        if records_to_insert:
            cur.executemany(INSERT_TEST_MOST_RUNS_SQL, records_to_insert)
            print(
                f"Successfully inserted {len(records_to_insert)} records into bcci_test_most_runs."
            )

"""## 2.6 Get the data from the database"""

# Configuration: Number of top players to display
TOP_N = 10

# Connect to the Database
with psycopg.connect(DATABASE_URL, autocommit=True) as conn:
    # Create a cursor and fetch data from both tables
    with conn.cursor() as cur:
        # Fetch ODI data ordered by rank (ascending, so rank 1 is best)
        cur.execute(
            """
              SELECT rank, name, matches, inns, avg, sr, hs,
                Fours, Sixes, Fifties, Centuries, runs
              FROM bcci_odi_most_runs
              ORDER BY rank ASC
              LIMIT %s
			      """,
            (TOP_N,),
        )

        odi_columns = [desc[0] for desc in cur.description]
        odi_data = cur.fetchall()

        # Fetch Test data ordered by rank (ascending, so rank 1 is best)
        cur.execute(
            """
              SELECT rank, name, matches, inns, avg, sr, hs,
                Fours, Sixes, Fifties, Centuries, runs
              FROM bcci_test_most_runs
              ORDER BY rank ASC
              LIMIT %s
			    """,
            (TOP_N,),
        )

        test_columns = [desc[0] for desc in cur.description]
        test_data = cur.fetchall()

"""## 2.7 Plot the runs comparision of the players"""

import matplotlib.pyplot as plt

# Convert fetched data (from the database) into pandas DataFrames for easier manipulation and plotting.
odi_df = pd.DataFrame(odi_data, columns=odi_columns)
test_df = pd.DataFrame(test_data, columns=test_columns)

# Common variables for charts
x_pos = range(len(odi_df)) # X-axis positions for the bars, based on the number of players
width = 0.35 # Width of each bar in the bar chart for side-by-side comparison

# --- First figure: Runs and Average Comparison ---
# Create a figure and a set of subplots (1 row, 2 columns) for these two charts.
fig1, axes1 = plt.subplots(1, 2, figsize=(16, 6))
# Set the overall title for the first figure.
fig1.suptitle(
    f"Top {TOP_N} Players Comparison: Test vs ODI - Runs & Average",
    fontsize=16,
    fontweight="bold",
)

# Chart 1: Total Runs Comparison
ax1 = axes1[0] # Select the first subplot for runs comparison
# Plot ODI runs: bars are offset slightly to the left of the center x_pos
ax1.bar(
    [x - width / 2 for x in x_pos],
    odi_df["runs"],
    width,
    label="ODI",
    color="#1f77b4",
    alpha=0.8,
)
# Plot Test runs: bars are offset slightly to the right of the center x_pos
ax1.bar(
    [x + width / 2 for x in x_pos],
    test_df["runs"],
    width,
    label="Test",
    color="#ff7f0e",
    alpha=0.8,
)
ax1.set_xlabel("Player")
ax1.set_ylabel("Runs")
ax1.set_title("Total Runs Comparison")
ax1.set_xticks(x_pos) # Set x-axis ticks at the player positions
ax1.set_xticklabels(odi_df["name"], rotation=90, ha="right") # Label x-axis with player names
ax1.legend()
ax1.grid(True, alpha=0.3)

# Chart 2: Batting Average Comparison
ax2 = axes1[1] # Select the second subplot for average comparison
# Plot ODI averages, offset to the left
ax2.bar(
    [x - width / 2 for x in x_pos],
    odi_df["avg"],
    width,
    label="ODI",
    color="#1f77b4",
    alpha=0.8,
)
# Plot Test averages, offset to the right
ax2.bar(
    [x + width / 2 for x in x_pos],
    test_df["avg"],
    width,
    label="Test",
    color="#ff7f0e",
    alpha=0.8,
)
ax2.set_xlabel("Player")
ax2.set_ylabel("Average")
ax2.set_title("Batting Average Comparison")
ax2.set_xticks(x_pos)
ax2.set_xticklabels(odi_df["name"], rotation=90, ha="right")
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.show()

# Second figure: Strike Rate and Centuries Comparison
fig2, axes2 = plt.subplots(1, 2, figsize=(16, 6))
fig2.suptitle(
    f"Top {TOP_N} Players Comparison: Test vs ODI - Strike Rate & Centuries",
    fontsize=16,
    fontweight="bold",
)

# Chart 3: Strike Rate Comparison
ax3 = axes2[0]
ax3.bar(
    [x - width / 2 for x in x_pos],
    odi_df["sr"],
    width,
    label="ODI",
    color="#1f77b4",
    alpha=0.8,
)
ax3.bar(
    [x + width / 2 for x in x_pos],
    test_df["sr"],
    width,
    label="Test",
    color="#ff7f0e",
    alpha=0.8,
)
ax3.set_xlabel("Player")
ax3.set_ylabel("Strike Rate")
ax3.set_title("Strike Rate Comparison")
ax3.set_xticks(x_pos)
ax3.set_xticklabels(odi_df["name"], rotation=90, ha="right")
ax3.legend()
ax3.grid(True, alpha=0.3)

# Chart 4: Centuries Comparison
ax4 = axes2[1]
ax4.bar(
    [x - width / 2 for x in x_pos],
    odi_df["centuries"],
    width,
    label="ODI",
    color="#1f77b4",
    alpha=0.8,
)
ax4.bar(
    [x + width / 2 for x in x_pos],
    test_df["centuries"],
    width,
    label="Test",
    color="#ff7f0e",
    alpha=0.8,
)
ax4.set_xlabel("Player")
ax4.set_ylabel("Centuries")
ax4.set_title("Centuries Comparison")
ax4.set_xticks(x_pos)
ax4.set_xticklabels(odi_df["name"], rotation=90, ha="right")
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.show()